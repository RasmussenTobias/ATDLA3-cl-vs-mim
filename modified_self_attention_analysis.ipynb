{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "921e17cb",
      "metadata": {
        "id": "921e17cb"
      },
      "source": [
        "# Self-Attention Analysis"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "6e66eb9f",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6e66eb9f",
        "outputId": "289f72c3-4b6c-49b2-8223-910e36751c77"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "\n",
        "#@markdown Execute this cell to import third-party software into the Colab environment.\n",
        "\n",
        "# check whether it runs in Colab\n",
        "root = \".\"\n",
        "if \"google.colab\" in sys.modules:\n",
        "    print(\"Running in Colab.\")\n",
        "    !pip3 install timm==0.5.4\n",
        "    !pip3 install matplotlib==3.7.1\n",
        "    !pip3 install scikit-learn==1.2.2\n",
        "    !pip3 install fastai==2.7.12\n",
        "    !pip3 install einops==0.6.0\n",
        "    !pip3 install gdown==4.7.1\n",
        "    !pip3 install yacs==0.1.8    \n",
        "    !git clone https://github.com/naver-ai/cl-vs-mim.git\n",
        "    root = \"./cl-vs-mim\"\n",
        "    %cd $root"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "203dfb55",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "203dfb55",
        "outputId": "05af1fa7-424e-4f96-a3d4-49e8fe49fc6d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "\n",
        "#@markdown Execute this cell to check the Colab environment, including GPU availability.\n",
        "\n",
        "# check gpu env\n",
        "print(f\"Torch: {torch.__version__} \\n\" + \n",
        "      f\"Availability: {torch.cuda.is_available()}\")\n",
        "assert torch.cuda.is_available() == True, \"The GPU is turned off. To turn it on, navigate to: Runtime > Change Runtime Type.\"\n",
        "print(f\"Number: {torch.cuda.device_count()} \\n\" +\n",
        "      f\"Current device: {torch.cuda.current_device()} \\n\" +\n",
        "      f\"First device: {torch.cuda.device(0)} \\n\" +\n",
        "      f\"Device name: {torch.cuda.get_device_name(0)}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fff99e10",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 37
        },
        "id": "fff99e10",
        "outputId": "5b985121-aea7-4509-c5c1-f3d09235d659"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import torch\n",
        "from torch.utils.data import DataLoader, Subset\n",
        "import torchvision.datasets as datasets\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "# Configuration\n",
        "DATASET = \"cifar100\"  # @param [\"cifar100\", \"flowers102\"]\n",
        "METHOD = \"full\"       # @param [\"full\", \"lora\", \"bitfit\", \"adaptformer\"]\n",
        "\n",
        "# Dataset configurations\n",
        "dataset_configs = {\n",
        "    \"cifar100\": {\n",
        "        \"num_classes\": 100,\n",
        "        \"data_dir\": \"./data/cifar-100-python\",\n",
        "        \"mean\": [0.485, 0.456, 0.406],\n",
        "        \"std\": [0.229, 0.224, 0.225]\n",
        "    },\n",
        "    \"flowers102\": {\n",
        "        \"num_classes\": 102,\n",
        "        \"data_dir\": \"./data/flowers102/dataset\",\n",
        "        \"mean\": [0.485, 0.456, 0.406], \n",
        "        \"std\": [0.229, 0.224, 0.225]\n",
        "    }\n",
        "}\n",
        "\n",
        "config = dataset_configs[DATASET]\n",
        "\n",
        "# Transform for analysis\n",
        "transform_test = transforms.Compose([\n",
        "    transforms.Resize(256),\n",
        "    transforms.CenterCrop(224),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(config[\"mean\"], config[\"std\"])\n",
        "])\n",
        "\n",
        "# Load dataset with proper error handling\n",
        "if DATASET == \"cifar100\":\n",
        "    dataset_full = datasets.CIFAR100(\n",
        "        root=config[\"data_dir\"], \n",
        "        train=False, \n",
        "        download=True, \n",
        "        transform=transform_test\n",
        "    )\n",
        "    print(f\"CIFAR-100 loaded: {len(dataset_full)} images\")\n",
        "elif DATASET == \"flowers102\":\n",
        "    test_dir = os.path.join(config[\"data_dir\"], \"valid\")\n",
        "    if not os.path.exists(test_dir):\n",
        "        print(f\"Directory not found: {test_dir}\")\n",
        "        # Try alternative paths\n",
        "        alt_paths = [\"./data/flowers102/dataset/valid\", \"./data/flowers102/valid\"]\n",
        "        for alt_path in alt_paths:\n",
        "            if os.path.exists(alt_path):\n",
        "                test_dir = alt_path\n",
        "                break\n",
        "    \n",
        "    dataset_full = datasets.ImageFolder(test_dir, transform=transform_test)\n",
        "    print(f\"Flowers102 loaded: {len(dataset_full)} images from {test_dir}\")\n",
        "\n",
        "# Better subsampling - use manual selection to ensure we get samples\n",
        "original_size = len(dataset_full)\n",
        "\n",
        "# For attention analysis, take every 8th sample (gives us ~128 samples from 1020)\n",
        "if original_size > 200:\n",
        "    step_size = max(1, original_size // 100)  # Aim for ~100 samples\n",
        "    indices = list(range(0, original_size, step_size))\n",
        "else:\n",
        "    indices = list(range(original_size))  # Use all if small dataset\n",
        "\n",
        "dataset_subset = Subset(dataset_full, indices)\n",
        "print(f\"Subsampled to {len(dataset_subset)} images (every {step_size if original_size > 200 else 1} samples)\")\n",
        "\n",
        "# Create DataLoader with reasonable batch size\n",
        "dataset_test = DataLoader(dataset_subset, num_workers=0, batch_size=16, shuffle=False)\n",
        "\n",
        "# Test the DataLoader\n",
        "try:\n",
        "    test_batch = next(iter(dataset_test))\n",
        "    print(f\"DataLoader working - batch shape: {test_batch[0].shape}\")\n",
        "    # Reset the iterator\n",
        "    dataset_test = DataLoader(dataset_subset, num_workers=0, batch_size=16, shuffle=False)\n",
        "except Exception as e:\n",
        "    print(f\"DataLoader error: {e}\")\n",
        "\n",
        "print(f\"Final dataset status: Ready with {len(dataset_subset)} samples\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "4532bc85",
      "metadata": {
        "id": "4532bc85"
      },
      "source": [
        "## Load the Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f959e0ff",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f959e0ff",
        "outputId": "b804d3eb-e7d5-4e3c-e4b9-0a6d3755b57d"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import ViTForImageClassification\n",
        "from models.lora_vit import apply_lora_to_vit\n",
        "from models.adapt_former import apply_adaptformer_to_vit\n",
        "from models.bitfit_vit import apply_bitfit_to_vit\n",
        "\n",
        "def load_finetuned_model(checkpoint_path, dataset_name, method=\"full\"):\n",
        "    \"\"\"Load a fine-tuned model from checkpoint with full method support\"\"\"\n",
        "    \n",
        "    if not os.path.exists(checkpoint_path):\n",
        "        print(f\"Checkpoint not found: {checkpoint_path}\")\n",
        "        return None\n",
        "    \n",
        "    num_classes_map = {\"cifar100\": 100, \"flowers102\": 102}\n",
        "    num_classes = num_classes_map[dataset_name]\n",
        "    \n",
        "    try:\n",
        "        # Create base model\n",
        "        model = ViTForImageClassification.from_pretrained(\n",
        "            \"facebook/dino-vits16\",\n",
        "            num_labels=num_classes,\n",
        "            ignore_mismatched_sizes=True,\n",
        "            output_attentions=True,\n",
        "            output_hidden_states=True\n",
        "        )\n",
        "        \n",
        "        # Apply method-specific modifications BEFORE loading weights\n",
        "        if method == \"lora\":\n",
        "            apply_lora_to_vit(model, r=8, alpha=16, dropout=0.1)\n",
        "            print(f\"Applied LoRA modifications\")\n",
        "        elif method == \"adaptformer\":\n",
        "            apply_adaptformer_to_vit(model, bottleneck_dim=64)\n",
        "            print(f\"Applied AdaptFormer modifications\")\n",
        "        elif method == \"bitfit\":\n",
        "            # BitFit modifications happen after loading\n",
        "            print(f\"BitFit will be applied after loading\")\n",
        "        \n",
        "        # Load the checkpoint\n",
        "        checkpoint = torch.load(checkpoint_path, map_location='cpu')\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        \n",
        "        # Apply BitFit after loading (it only changes requires_grad flags)\n",
        "        if method == \"bitfit\":\n",
        "            apply_bitfit_to_vit(model, verbose=False)\n",
        "        \n",
        "        model = model.cuda().eval()\n",
        "        print(f\"Loaded: {dataset_name}_{method}\")\n",
        "        return model\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"Error loading {checkpoint_path}: {e}\")\n",
        "        return None\n",
        "\n",
        "# Test loading all models\n",
        "print(\"Testing model loading...\")\n",
        "models = {}\n",
        "methods = [\"full\", \"lora\", \"bitfit\", \"adaptformer\"]\n",
        "datasets = [\"cifar100\", \"flowers102\"]\n",
        "\n",
        "for dataset in datasets:\n",
        "    for method in methods:\n",
        "        checkpoint_path = f\"./checkpoints/{dataset}/{method}/best_checkpoint_{dataset}_{method}.pth\"\n",
        "        model_key = f\"{dataset}_{method}\"\n",
        "        models[model_key] = load_finetuned_model(checkpoint_path, dataset, method)\n",
        "\n",
        "# Filter and wrap successful models\n",
        "available_models = {}\n",
        "for k, v in models.items():\n",
        "    if v is not None:\n",
        "        available_models[k] = create_analysis_wrapper(v)\n",
        "\n",
        "print(f\"\\nAvailable models: {list(available_models.keys())}\")\n",
        "\n",
        "# Select model for analysis\n",
        "current_model_key = f\"{DATASET}_{METHOD}\"\n",
        "if current_model_key in available_models:\n",
        "    selected_model = available_models[current_model_key]\n",
        "    print(f\"Selected for analysis: {current_model_key}\")\n",
        "else:\n",
        "    print(f\"Selected model {current_model_key} not available\")\n",
        "    if available_models:\n",
        "        fallback_key = list(available_models.keys())[0]\n",
        "        selected_model = available_models[fallback_key]\n",
        "        print(f\"Using fallback: {fallback_key}\")\n",
        "    else:\n",
        "        selected_model = None\n",
        "        print(\"No models available for analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7e1b1d97",
      "metadata": {
        "id": "7e1b1d97"
      },
      "source": [
        "## Attention Map Visualization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "17b41b3c",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 336
        },
        "id": "17b41b3c",
        "outputId": "a1bdfb4a-f8d4-490a-c618-51c878518e07",
        "scrolled": false
      },
      "outputs": [],
      "source": [
        "# Cell: Attention Map Visualization - Enhanced Error Handling\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def mark_token(ax, xs, batch, i, j, color='tab:red'):\n",
        "    import matplotlib.patches as patches\n",
        "    from einops import rearrange\n",
        "    \n",
        "    # Denormalize for visualization - ensure all tensors are on the same device\n",
        "    device = xs.device\n",
        "    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1).to(device)\n",
        "    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1).to(device)\n",
        "    xs_denorm = xs * std + mean\n",
        "    xs_denorm = torch.clamp(xs_denorm, 0, 1)\n",
        "    \n",
        "    # Move to CPU for visualization\n",
        "    xs_denorm_cpu = xs_denorm.cpu()\n",
        "    \n",
        "    ax.imshow(rearrange(xs_denorm_cpu[batch], 'c h w -> h w c'))\n",
        "    ax.set_axis_off()\n",
        "\n",
        "    # Create rectangle patch to visualize the query token\n",
        "    rect = patches.Rectangle((16 * i, 16 * j), 16, 16, linewidth=3, edgecolor=color, facecolor='none')\n",
        "    ax.add_patch(rect)\n",
        "\n",
        "def visualize_attn(ax, attns, batch, i, j, depth, head):\n",
        "    \"\"\"Visualize attention maps for fine-tuned models\"\"\"\n",
        "        \n",
        "    if depth >= len(attns) or attns[depth] is None:\n",
        "        ax.text(0.5, 0.5, f\"No attention data at layer {depth}\", ha='center', va='center', transform=ax.transAxes)\n",
        "        print(f\"   - Available layers: {[i for i, a in enumerate(attns) if a is not None]}\")\n",
        "        return\n",
        "    \n",
        "    attn = attns[depth]\n",
        "    \n",
        "    if batch >= attn.shape[0]:\n",
        "        ax.text(0.5, 0.5, f\"Batch {batch} out of range (max: {attn.shape[0]-1})\", ha='center', va='center', transform=ax.transAxes)\n",
        "        return\n",
        "    \n",
        "    attn_batch = attn[batch]  # Get attention for specific batch\n",
        "    print(f\"Batch attention shape: {attn_batch.shape}\")\n",
        "    \n",
        "    if attn_batch.dim() == 3:  # (num_heads, seq_len, seq_len)\n",
        "        if head >= attn_batch.shape[0]:\n",
        "            ax.text(0.5, 0.5, f\"Head {head} out of range (max: {attn_batch.shape[0]-1})\", ha='center', va='center', transform=ax.transAxes)\n",
        "            return\n",
        "        attn_head = attn_batch[head]  # Select specific head\n",
        "        print(f\"Head attention shape: {attn_head.shape}\")\n",
        "    elif attn_batch.dim() == 2:  # Already (seq_len, seq_len)\n",
        "        attn_head = attn_batch\n",
        "        print(f\"Using 2D attention directly: {attn_head.shape}\")\n",
        "    else:\n",
        "        ax.text(0.5, 0.5, f\"Unexpected attention shape: {attn_batch.shape}\", ha='center', va='center', transform=ax.transAxes)\n",
        "        return\n",
        "    \n",
        "    # Remove CLS token\n",
        "    attn_no_cls = attn_head[1:, 1:]  \n",
        "    print(f\"After CLS removal: {attn_no_cls.shape}\")\n",
        "    \n",
        "    # Normalize\n",
        "    attn_norm = attn_no_cls / attn_no_cls.sum(dim=-1, keepdim=True)\n",
        "    \n",
        "    # Get attention for specific query token\n",
        "    seq_len = int(math.sqrt(attn_norm.shape[0]))\n",
        "    print(f\"Sequence length (sqrt): {seq_len}\")\n",
        "    \n",
        "    if seq_len * seq_len != attn_norm.shape[0]:\n",
        "        ax.text(0.5, 0.5, f\"Non-square attention: {attn_norm.shape[0]} != {seq_len}²\", ha='center', va='center', transform=ax.transAxes)\n",
        "        return\n",
        "    \n",
        "    query_idx = j * seq_len + i\n",
        "    print(f\"Query index: {query_idx} (i={i}, j={j})\")\n",
        "    \n",
        "    if query_idx >= attn_norm.shape[0]:\n",
        "        ax.text(0.5, 0.5, f\"Query {query_idx} out of range (max: {attn_norm.shape[0]-1})\", ha='center', va='center', transform=ax.transAxes)\n",
        "        return\n",
        "        \n",
        "    attn_map = attn_norm[query_idx].reshape(seq_len, seq_len)\n",
        "    print(f\"Final attention map shape: {attn_map.shape}\")\n",
        "    \n",
        "    ax.imshow(attn_map.cpu().numpy(), cmap=\"plasma\")\n",
        "    ax.set_xticks([])\n",
        "    ax.set_yticks([])\n",
        "    ax.set_axis_off()\n",
        "\n",
        "# Run attention visualization with better error handling\n",
        "if selected_model is not None:\n",
        "    print(f\"Running Attention Visualization for {DATASET}_{METHOD}...\")\n",
        "    \n",
        "    # Check if dataset has any data first\n",
        "    try:\n",
        "        xs, ys = next(iter(dataset_test))\n",
        "        print(f\"Got batch - xs: {xs.shape}, ys: {len(ys)}\")\n",
        "    except StopIteration:\n",
        "        print(\"Dataset is empty - no data available for visualization\")\n",
        "        print(\"Please run the dataset loading cell again with proper subsampling\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error getting data: {e}\")\n",
        "    else:\n",
        "        # Parameters for visualization\n",
        "        depth = min(10, 11)  # Layer depth (0-11 for ViT-Base)\n",
        "        batch = 0            # Use first batch to avoid issues\n",
        "        coord_x = 3          # Query token x coordinate\n",
        "        coord_y = 8          # Query token y coordinate  \n",
        "        head = 0             # Attention head\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            xs = xs.cuda()                      \n",
        "            \n",
        "            _, _, attns, _ = selected_model.forward_analysis(xs)\n",
        "            print(f\"Wrapper attention layers: {len(attns)}\")\n",
        "            print(f\"Non-None layers: {[i for i, a in enumerate(attns) if a is not None]}\")\n",
        "        \n",
        "        # Adjust parameters based on actual data\n",
        "        if len(attns) > 0:\n",
        "            # Find a valid layer with attention data\n",
        "            valid_layers = [i for i, a in enumerate(attns) if a is not None]\n",
        "            if valid_layers:\n",
        "                depth = valid_layers[-1]  # Use the last valid layer\n",
        "                print(f\"Using layer {depth} for visualization\")\n",
        "            else:\n",
        "                print(\"No valid attention layers found\")\n",
        "                depth = None\n",
        "        \n",
        "        # Adjust batch size\n",
        "        batch = min(batch, xs.shape[0] - 1)\n",
        "        print(f\"Using batch index: {batch}\")\n",
        "        \n",
        "        # Visualize original image with query token marked\n",
        "        fig, ax = plt.subplots(1, 1, figsize=(4, 4), dpi=100)\n",
        "        mark_token(ax, xs, batch=batch, i=coord_x, j=coord_y, color='tab:red')\n",
        "        ax.set_title(f\"Query Token Location - {DATASET.upper()} ({METHOD.upper()})\")\n",
        "        plt.show()\n",
        "        \n",
        "        # Visualize attention map\n",
        "        if depth is not None:\n",
        "            fig, ax = plt.subplots(1, 1, figsize=(4, 4), dpi=150)\n",
        "            visualize_attn(ax, attns, batch=batch, i=coord_x, j=coord_y, depth=depth, head=head)\n",
        "            ax.set_title(f\"Attention Map (Layer {depth}, Head {head})\")\n",
        "            plt.show()\n",
        "        else:\n",
        "            print(\"Cannot visualize attention - no valid layers\")\n",
        "    \n",
        "else:\n",
        "    print(\"No model available for attention visualization\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "b7f8f456",
      "metadata": {
        "id": "b7f8f456"
      },
      "source": [
        "## Attention Distance"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7dad3f9d",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "7dad3f9d",
        "outputId": "32c49d26-0d97-4ea6-f1b7-39a6dcf20c80"
      },
      "outputs": [],
      "source": [
        "# Cell: Attention Distance Analysis - Updated for Fine-tuned Models\n",
        "import numpy as np\n",
        "from timm.utils import AverageMeter\n",
        "\n",
        "def compute_distance_matrix(patch_size, num_patches, length):\n",
        "    \"\"\"Helper function to compute distance matrix.\"\"\"\n",
        "    distance_matrix = np.zeros((num_patches, num_patches))\n",
        "    for i in range(num_patches):\n",
        "        for j in range(num_patches):\n",
        "            if i == j:\n",
        "                continue\n",
        "            xi, yi = (int(i/length)), (i % length)\n",
        "            xj, yj = (int(j/length)), (j % length)\n",
        "            distance_matrix[i, j] = patch_size * np.linalg.norm([xi - xj, yi - yj])\n",
        "    return distance_matrix\n",
        "\n",
        "def calculate_mean_attention_dist(patch_size, attention_weights):\n",
        "    \"\"\"Calculate mean attention distance\"\"\"\n",
        "    num_patches = attention_weights.shape[-1]\n",
        "    length = int(np.sqrt(num_patches))\n",
        "    assert (length**2 == num_patches), (\"Num patches is not perfect square\")\n",
        "\n",
        "    distance_matrix = compute_distance_matrix(patch_size, num_patches, length)\n",
        "    h, w = distance_matrix.shape\n",
        "    distance_matrix = distance_matrix.reshape((1, 1, h, w))\n",
        "    \n",
        "    mean_distances = attention_weights * distance_matrix\n",
        "    mean_distances = np.sum(mean_distances, axis=-1)\n",
        "    mean_distances = np.mean(mean_distances, axis=-1)\n",
        "    return torch.tensor(mean_distances)\n",
        "\n",
        "# Run attention distance analysis\n",
        "if selected_model is not None:\n",
        "    print(f\"Running Attention Distance Analysis for {DATASET}_{METHOD}...\")\n",
        "    \n",
        "    encoder_length = 12  # ViT-Base has 12 layers\n",
        "    distances = [AverageMeter() for _ in range(encoder_length)]\n",
        "    \n",
        "    for idx, (xs, _) in enumerate(dataset_test):\n",
        "        xs = xs.cuda()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            _, _, attns, _ = selected_model.forward_analysis(xs)\n",
        "        \n",
        "        for i, attn in enumerate(attns):\n",
        "            if attn is not None:\n",
        "                # Handle different attention formats\n",
        "                if attn.dim() == 4:  # (batch, heads, seq, seq)\n",
        "                    attn_processed = attn[:, :, 1:, 1:]  # Remove CLS token\n",
        "                elif attn.dim() == 3:  # (batch, seq, seq) - single head\n",
        "                    attn_processed = attn[:, 1:, 1:].unsqueeze(1)  # Add head dimension\n",
        "                else:\n",
        "                    continue\n",
        "                \n",
        "                attn_processed = attn_processed + 1e-32\n",
        "                attn_processed = attn_processed / attn_processed.sum(dim=-1, keepdim=True)\n",
        "                attn_processed = attn_processed.cpu().float().detach().numpy()\n",
        "                \n",
        "                distance = calculate_mean_attention_dist(patch_size=16, attention_weights=attn_processed)\n",
        "                distances[i].update(torch.mean(distance, dim=0))\n",
        "        \n",
        "        if idx >= 5:  # Process a few batches\n",
        "            break\n",
        "    \n",
        "    # Plot results\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(6, 5), dpi=150)\n",
        "    valid_distances = []\n",
        "    valid_layers = []\n",
        "    \n",
        "    for i, distance in enumerate(distances):\n",
        "        if distance.count > 0:\n",
        "            valid_distances.append(torch.mean(distance.avg).item())\n",
        "            valid_layers.append(i + 1)\n",
        "    \n",
        "    if valid_distances:\n",
        "        ax.plot(valid_layers, valid_distances, marker=\"o\", linewidth=2, markersize=6)\n",
        "        ax.set_xlabel(\"Layer Depth\", fontsize=12)\n",
        "        ax.set_ylabel(\"Attention Distance (px)\", fontsize=12)\n",
        "        ax.set_title(f\"Attention Distance - {DATASET.upper()} ({METHOD.upper()})\", fontsize=14, fontweight='bold')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.set_ylim(bottom=0)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No valid attention distance data collected\")\n",
        "        \n",
        "else:\n",
        "    print(\"No model available for attention distance analysis\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "34e8593d",
      "metadata": {
        "id": "34e8593d"
      },
      "source": [
        "## Normalized Mutual Information"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7cfc022f",
      "metadata": {
        "cellView": "form",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 579
        },
        "id": "7cfc022f",
        "outputId": "50c84d60-6e1c-4b61-ff90-8546d8ca038a"
      },
      "outputs": [],
      "source": [
        "# Cell: Normalized Mutual Information Analysis - Updated for Fine-tuned Models\n",
        "import torch.nn.functional as F \n",
        "from einops import rearrange, reduce, repeat\n",
        "\n",
        "def calculate_nmi(attn): \n",
        "    \"\"\"Normalized mutual information with return type of (batch, head)\"\"\"\n",
        "    b, h, q, k = attn.shape\n",
        "    pq = torch.ones([b, h, q]).to(attn.device)\n",
        "    pq = F.softmax(pq, dim=-1)\n",
        "    pq_ext = repeat(pq, \"b h q -> b h q k\", k=k)\n",
        "    pk = reduce(attn * pq_ext, \"b h q k -> b h k\", \"sum\")\n",
        "    pk_ext = repeat(pk, \"b h k -> b h q k\", q=q)\n",
        "    \n",
        "    mi = reduce(attn * pq_ext * torch.log(attn / pk_ext), \"b h q k -> b h\", \"sum\")\n",
        "    eq = - reduce(pq * torch.log(pq), \"b h q -> b h\", \"sum\")\n",
        "    ek = - reduce(pk * torch.log(pk), \"b h k -> b h\", \"sum\")\n",
        "    \n",
        "    nmiv = mi / torch.sqrt(eq * ek)\n",
        "    return nmiv\n",
        "\n",
        "# Run NMI analysis\n",
        "if selected_model is not None:\n",
        "    print(f\"Running Normalized Mutual Information Analysis for {DATASET}_{METHOD}...\")\n",
        "    \n",
        "    encoder_length = 12  # ViT-Base has 12 layers\n",
        "    nmis = [AverageMeter() for _ in range(encoder_length)]\n",
        "    \n",
        "    for idx, (xs, _) in enumerate(dataset_test):\n",
        "        xs = xs.cuda()\n",
        "        \n",
        "        with torch.no_grad():\n",
        "            _, _, attns, _ = selected_model.forward_analysis(xs)\n",
        "        \n",
        "        for i, attn in enumerate(attns):\n",
        "            if attn is not None:\n",
        "                # Handle different attention formats\n",
        "                if attn.dim() == 4:  # (batch, heads, seq, seq)\n",
        "                    attn_processed = attn[:, :, 1:, 1:]  # Remove CLS token\n",
        "                elif attn.dim() == 3:  # (batch, seq, seq) - single head\n",
        "                    attn_processed = attn[:, 1:, 1:].unsqueeze(1)  # Add head dimension\n",
        "                else:\n",
        "                    continue\n",
        "                \n",
        "                attn_processed = attn_processed + 1e-8\n",
        "                attn_processed = attn_processed / attn_processed.sum(dim=-1, keepdim=True)\n",
        "                attn_processed = attn_processed.cpu().float()\n",
        "                \n",
        "                nmi = calculate_nmi(attn_processed)\n",
        "                nmis[i].update(torch.mean(nmi, dim=0))\n",
        "        \n",
        "        if idx >= 5:  # Process a few batches\n",
        "            break\n",
        "    \n",
        "    # Plot results\n",
        "    fig, ax = plt.subplots(1, 1, figsize=(6, 5), dpi=150)\n",
        "    valid_nmis = []\n",
        "    valid_layers = []\n",
        "    \n",
        "    for i, nmi in enumerate(nmis):\n",
        "        if nmi.count > 0:\n",
        "            valid_nmis.append(torch.mean(nmi.avg).item())\n",
        "            valid_layers.append(i + 1)\n",
        "    \n",
        "    if valid_nmis:\n",
        "        ax.plot(valid_layers, valid_nmis, marker=\"o\", linewidth=2, markersize=6)\n",
        "        ax.set_xlabel(\"Layer Depth\", fontsize=12)\n",
        "        ax.set_ylabel(\"Normalized Mutual Information\", fontsize=12)\n",
        "        ax.set_title(f\"Attention NMI - {DATASET.upper()} ({METHOD.upper()})\", fontsize=14, fontweight='bold')\n",
        "        ax.grid(True, alpha=0.3)\n",
        "        ax.set_ylim(bottom=0)\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "    else:\n",
        "        print(\"No valid NMI data collected\")\n",
        "        \n",
        "else:\n",
        "    print(\"No model available for NMI analysis\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdb85c84",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell: Comparative Attention Distance Analysis - All Models\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from timm.utils import AverageMeter\n",
        "\n",
        "def compute_distance_matrix(patch_size, num_patches, length):\n",
        "    \"\"\"Helper function to compute distance matrix.\"\"\"\n",
        "    distance_matrix = np.zeros((num_patches, num_patches))\n",
        "    for i in range(num_patches):\n",
        "        for j in range(num_patches):\n",
        "            if i == j:\n",
        "                continue\n",
        "            xi, yi = (int(i/length)), (i % length)\n",
        "            xj, yj = (int(j/length)), (j % length)\n",
        "            distance_matrix[i, j] = patch_size * np.linalg.norm([xi - xj, yi - yj])\n",
        "    return distance_matrix\n",
        "\n",
        "def calculate_mean_attention_dist(patch_size, attention_weights):\n",
        "    \"\"\"Calculate mean attention distance\"\"\"\n",
        "    num_patches = attention_weights.shape[-1]\n",
        "    length = int(np.sqrt(num_patches))\n",
        "    assert (length**2 == num_patches), (\"Num patches is not perfect square\")\n",
        "\n",
        "    distance_matrix = compute_distance_matrix(patch_size, num_patches, length)\n",
        "    h, w = distance_matrix.shape\n",
        "    distance_matrix = distance_matrix.reshape((1, 1, h, w))\n",
        "    \n",
        "    mean_distances = attention_weights * distance_matrix\n",
        "    mean_distances = np.sum(mean_distances, axis=-1)\n",
        "    mean_distances = np.mean(mean_distances, axis=-1)\n",
        "    return torch.tensor(mean_distances)\n",
        "\n",
        "def analyze_model_attention_distance(model, dataset_loader, model_name, max_batches=5):\n",
        "    \"\"\"Analyze attention distance for a single model\"\"\"\n",
        "    print(f\"Analyzing attention distance for {model_name}...\")\n",
        "    \n",
        "    encoder_length = 12  # ViT-Base has 12 layers\n",
        "    distances = [AverageMeter() for _ in range(encoder_length)]\n",
        "    \n",
        "    for idx, (xs, _) in enumerate(dataset_loader):\n",
        "        try:\n",
        "            xs = xs.cuda()\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                _, _, attns, _ = model.forward_analysis(xs)\n",
        "            \n",
        "            for i, attn in enumerate(attns):\n",
        "                if attn is not None:\n",
        "                    # Handle different attention formats\n",
        "                    if attn.dim() == 4:  # (batch, heads, seq, seq)\n",
        "                        attn_processed = attn[:, :, 1:, 1:]  # Remove CLS token\n",
        "                    elif attn.dim() == 3:  # (batch, seq, seq) - single head\n",
        "                        attn_processed = attn[:, 1:, 1:].unsqueeze(1)  # Add head dimension\n",
        "                    else:\n",
        "                        continue\n",
        "                    \n",
        "                    attn_processed = attn_processed + 1e-32\n",
        "                    attn_processed = attn_processed / attn_processed.sum(dim=-1, keepdim=True)\n",
        "                    attn_processed = attn_processed.cpu().float().detach().numpy()\n",
        "                    \n",
        "                    distance = calculate_mean_attention_dist(patch_size=16, attention_weights=attn_processed)\n",
        "                    distances[i].update(torch.mean(distance, dim=0))\n",
        "            \n",
        "            if idx >= max_batches:\n",
        "                break\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch {idx}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Extract valid distances\n",
        "    valid_distances = []\n",
        "    valid_layers = []\n",
        "    \n",
        "    for i, distance in enumerate(distances):\n",
        "        if distance.count > 0:\n",
        "            valid_distances.append(torch.mean(distance.avg).item())\n",
        "            valid_layers.append(i + 1)\n",
        "    \n",
        "    if valid_distances:\n",
        "        print(f\"Collected distance data for {len(valid_distances)} layers\")\n",
        "        return valid_layers, valid_distances\n",
        "    else:\n",
        "        print(f\"   ❌ No valid distance data collected\")\n",
        "        return None, None\n",
        "\n",
        "# Run comparative attention distance analysis\n",
        "if available_models and dataset_test is not None:\n",
        "    print(f\"Running Comparative Attention Distance Analysis for {DATASET.upper()}...\")\n",
        "    \n",
        "    # Define visual styles for each method\n",
        "    method_styles = {\n",
        "        'full': {'marker': 'o', 'color': '#1f77b4', 'linestyle': '-', 'label': 'Full Fine-tuning'},\n",
        "        'lora': {'marker': 's', 'color': '#ff7f0e', 'linestyle': '--', 'label': 'LoRA'},\n",
        "        'bitfit': {'marker': '^', 'color': '#2ca02c', 'linestyle': '-.', 'label': 'BitFit'},\n",
        "        'adaptformer': {'marker': 'D', 'color': '#d62728', 'linestyle': ':', 'label': 'AdaptFormer'}\n",
        "    }\n",
        "    \n",
        "    # Collect results for all models\n",
        "    results = {}\n",
        "    \n",
        "    for model_key, model in available_models.items():\n",
        "        dataset_name, method = model_key.split('_')\n",
        "        \n",
        "        # Only analyze models for the current dataset\n",
        "        if dataset_name == DATASET:\n",
        "            layers, distances = analyze_model_attention_distance(model, dataset_test, model_key)\n",
        "            if layers is not None and distances is not None:\n",
        "                results[method] = (layers, distances)\n",
        "    \n",
        "    # Plot comparative results\n",
        "    if results:\n",
        "        print(f\"\\nPlotting comparative attention distance results...\")\n",
        "        \n",
        "        fig, ax = plt.subplots(1, 1, figsize=(10, 6), dpi=150)\n",
        "        \n",
        "        for method, (layers, distances) in results.items():\n",
        "            style = method_styles.get(method, {'marker': 'o', 'color': 'black', 'linestyle': '-', 'label': method})\n",
        "            \n",
        "            ax.plot(layers, distances, \n",
        "                   marker=style['marker'], \n",
        "                   color=style['color'],\n",
        "                   linestyle=style['linestyle'],\n",
        "                   linewidth=2.5, \n",
        "                   markersize=8,\n",
        "                   markerfacecolor='white',\n",
        "                   markeredgecolor=style['color'],\n",
        "                   markeredgewidth=2,\n",
        "                   label=style['label'],\n",
        "                   alpha=0.8)\n",
        "        \n",
        "        # Customize the plot\n",
        "        ax.set_xlabel(\"Layer Depth\", fontsize=14, fontweight='bold')\n",
        "        ax.set_ylabel(\"Attention Distance (px)\", fontsize=14, fontweight='bold')\n",
        "        ax.set_title(f\"Attention Distance Comparison - {DATASET.upper()}\", \n",
        "                    fontsize=16, fontweight='bold', pad=20)\n",
        "        \n",
        "        # Add grid and legend\n",
        "        ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)\n",
        "        ax.legend(fontsize=12, frameon=True, fancybox=True, shadow=True, \n",
        "                 bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        \n",
        "        # Set axis limits and styling\n",
        "        ax.set_ylim(bottom=0)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=11)\n",
        "        \n",
        "        # Add subtle background\n",
        "        ax.set_facecolor('#fafafa')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Print summary statistics\n",
        "        print(\"\\nAttention Distance Summary Statistics:\")\n",
        "        print(\"=\" * 60)\n",
        "        for method, (layers, distances) in results.items():\n",
        "            distances_array = np.array(distances)\n",
        "            print(f\"{method_styles[method]['label']:15} | \"\n",
        "                  f\"Mean: {distances_array.mean():.2f}px | \"\n",
        "                  f\"Std: {distances_array.std():.2f}px | \"\n",
        "                  f\"Min: {distances_array.min():.2f}px | \"\n",
        "                  f\"Max: {distances_array.max():.2f}px\")\n",
        "        \n",
        "    else:\n",
        "        print(\"No valid attention distance results collected for comparison\")\n",
        "        \n",
        "else:\n",
        "    print(\"Cannot run comparative attention distance analysis - missing models or dataset\")\n",
        "    print(f\"Available models: {list(available_models.keys()) if available_models else 'None'}\")\n",
        "    print(f\"Dataset available: {dataset_test is not None}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "b0a9f4ce",
      "metadata": {},
      "outputs": [],
      "source": [
        "# Cell: Comparative Attention NMI Analysis - All Models\n",
        "import matplotlib.pyplot as plt\n",
        "import torch.nn.functional as F \n",
        "from einops import rearrange, reduce, repeat\n",
        "from timm.utils import AverageMeter\n",
        "import numpy as np\n",
        "\n",
        "def calculate_nmi(attn): \n",
        "    \"\"\"Normalized mutual information with return type of (batch, head)\"\"\"\n",
        "    b, h, q, k = attn.shape\n",
        "    pq = torch.ones([b, h, q]).to(attn.device)\n",
        "    pq = F.softmax(pq, dim=-1)\n",
        "    pq_ext = repeat(pq, \"b h q -> b h q k\", k=k)\n",
        "    pk = reduce(attn * pq_ext, \"b h q k -> b h k\", \"sum\")\n",
        "    pk_ext = repeat(pk, \"b h k -> b h q k\", q=q)\n",
        "    \n",
        "    mi = reduce(attn * pq_ext * torch.log(attn / pk_ext), \"b h q k -> b h\", \"sum\")\n",
        "    eq = - reduce(pq * torch.log(pq), \"b h q -> b h\", \"sum\")\n",
        "    ek = - reduce(pk * torch.log(pk), \"b h k -> b h\", \"sum\")\n",
        "    \n",
        "    nmiv = mi / torch.sqrt(eq * ek)\n",
        "    return nmiv\n",
        "\n",
        "def analyze_model_attention_nmi(model, dataset_loader, model_name, max_batches=5):\n",
        "    \"\"\"Analyze attention NMI for a single model\"\"\"\n",
        "    print(f\"Analyzing attention NMI for {model_name}...\")\n",
        "    \n",
        "    encoder_length = 12  # ViT-Base has 12 layers\n",
        "    nmis = [AverageMeter() for _ in range(encoder_length)]\n",
        "    \n",
        "    for idx, (xs, _) in enumerate(dataset_loader):\n",
        "        try:\n",
        "            xs = xs.cuda()\n",
        "            \n",
        "            with torch.no_grad():\n",
        "                _, _, attns, _ = model.forward_analysis(xs)\n",
        "            \n",
        "            for i, attn in enumerate(attns):\n",
        "                if attn is not None:\n",
        "                    # Handle different attention formats\n",
        "                    if attn.dim() == 4:  # (batch, heads, seq, seq)\n",
        "                        attn_processed = attn[:, :, 1:, 1:]  # Remove CLS token\n",
        "                    elif attn.dim() == 3:  # (batch, seq, seq) - single head\n",
        "                        attn_processed = attn[:, 1:, 1:].unsqueeze(1)  # Add head dimension\n",
        "                    else:\n",
        "                        continue\n",
        "                    \n",
        "                    attn_processed = attn_processed + 1e-8\n",
        "                    attn_processed = attn_processed / attn_processed.sum(dim=-1, keepdim=True)\n",
        "                    attn_processed = attn_processed.cpu().float()\n",
        "                    \n",
        "                    nmi = calculate_nmi(attn_processed)\n",
        "                    nmis[i].update(torch.mean(nmi, dim=0))\n",
        "            \n",
        "            if idx >= max_batches:\n",
        "                break\n",
        "                \n",
        "        except Exception as e:\n",
        "            print(f\"Error processing batch {idx}: {e}\")\n",
        "            continue\n",
        "    \n",
        "    # Extract valid NMI values\n",
        "    valid_nmis = []\n",
        "    valid_layers = []\n",
        "    \n",
        "    for i, nmi in enumerate(nmis):\n",
        "        if nmi.count > 0:\n",
        "            valid_nmis.append(torch.mean(nmi.avg).item())\n",
        "            valid_layers.append(i + 1)\n",
        "    \n",
        "    if valid_nmis:\n",
        "        print(f\"Collected NMI data for {len(valid_nmis)} layers\")\n",
        "        return valid_layers, valid_nmis\n",
        "    else:\n",
        "        print(f\"No valid NMI data collected\")\n",
        "        return None, None\n",
        "\n",
        "# Run comparative attention NMI analysis\n",
        "if available_models and dataset_test is not None:\n",
        "    print(f\"Running Comparative Attention NMI Analysis for {DATASET.upper()}...\")\n",
        "    \n",
        "    # Define visual styles for each method (same as previous)\n",
        "    method_styles = {\n",
        "        'full': {'marker': 'o', 'color': '#1f77b4', 'linestyle': '-', 'label': 'Full Fine-tuning'},\n",
        "        'lora': {'marker': 's', 'color': '#ff7f0e', 'linestyle': '--', 'label': 'LoRA'},\n",
        "        'bitfit': {'marker': '^', 'color': '#2ca02c', 'linestyle': '-.', 'label': 'BitFit'},\n",
        "        'adaptformer': {'marker': 'D', 'color': '#d62728', 'linestyle': ':', 'label': 'AdaptFormer'}\n",
        "    }\n",
        "    \n",
        "    # Collect results for all models\n",
        "    results = {}\n",
        "    \n",
        "    for model_key, model in available_models.items():\n",
        "        dataset_name, method = model_key.split('_')\n",
        "        \n",
        "        # Only analyze models for the current dataset\n",
        "        if dataset_name == DATASET:\n",
        "            layers, nmis = analyze_model_attention_nmi(model, dataset_test, model_key)\n",
        "            if layers is not None and nmis is not None:\n",
        "                results[method] = (layers, nmis)\n",
        "    \n",
        "    # Plot comparative results\n",
        "    if results:\n",
        "        print(f\"\\nPlotting comparative attention NMI results...\")\n",
        "        \n",
        "        fig, ax = plt.subplots(1, 1, figsize=(10, 6), dpi=150)\n",
        "        \n",
        "        for method, (layers, nmis) in results.items():\n",
        "            style = method_styles.get(method, {'marker': 'o', 'color': 'black', 'linestyle': '-', 'label': method})\n",
        "            \n",
        "            ax.plot(layers, nmis, \n",
        "                   marker=style['marker'], \n",
        "                   color=style['color'],\n",
        "                   linestyle=style['linestyle'],\n",
        "                   linewidth=2.5, \n",
        "                   markersize=8,\n",
        "                   markerfacecolor='white',\n",
        "                   markeredgecolor=style['color'],\n",
        "                   markeredgewidth=2,\n",
        "                   label=style['label'],\n",
        "                   alpha=0.8)\n",
        "        \n",
        "        # Customize the plot\n",
        "        ax.set_xlabel(\"Layer Depth\", fontsize=14, fontweight='bold')\n",
        "        ax.set_ylabel(\"Normalized Mutual Information\", fontsize=14, fontweight='bold')\n",
        "        ax.set_title(f\"Attention NMI Comparison - {DATASET.upper()}\", \n",
        "                    fontsize=16, fontweight='bold', pad=20)\n",
        "        \n",
        "        # Add grid and legend\n",
        "        ax.grid(True, alpha=0.3, linestyle='-', linewidth=0.5)\n",
        "        ax.legend(fontsize=12, frameon=True, fancybox=True, shadow=True, \n",
        "                 bbox_to_anchor=(1.05, 1), loc='upper left')\n",
        "        \n",
        "        # Set axis limits and styling\n",
        "        ax.set_ylim(bottom=0)\n",
        "        ax.tick_params(axis='both', which='major', labelsize=11)\n",
        "        \n",
        "        # Add subtle background\n",
        "        ax.set_facecolor('#fafafa')\n",
        "        \n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "        \n",
        "        # Print summary statistics\n",
        "        print(\"\\nAttention NMI Summary Statistics:\")\n",
        "        print(\"=\" * 60)\n",
        "        for method, (layers, nmis) in results.items():\n",
        "            nmis_array = np.array(nmis)\n",
        "            print(f\"{method_styles[method]['label']:15} | \"\n",
        "                  f\"Mean: {nmis_array.mean():.4f} | \"\n",
        "                  f\"Std: {nmis_array.std():.4f} | \"\n",
        "                  f\"Min: {nmis_array.min():.4f} | \"\n",
        "                  f\"Max: {nmis_array.max():.4f}\")\n",
        "        \n",
        "    else:\n",
        "        print(\"No valid attention NMI results collected for comparison\")\n",
        "        \n",
        "else:\n",
        "    print(\"Cannot run comparative attention NMI analysis - missing models or dataset\")\n",
        "    print(f\"Available models: {list(available_models.keys()) if available_models else 'None'}\")\n",
        "    print(f\"Dataset available: {dataset_test is not None}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "vitlora",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
